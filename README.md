# Lokalny Chatbot AI z Interfejsem Webowym

Jest to w peni funkcjonalna aplikacja typu "full-stack", kt贸ra pozwala na prowadzenie rozm贸w z lokalnym modelem jzykowym (AI) za porednictwem nowoczesnego interfejsu webowego. Backend aplikacji zosta zbudowany w Pythonie przy u偶yciu FastAPI, a frontend w React (TypeScript) i Tailwind CSS.


---

## G贸wne Funkcje

*    **Lokalny Model AI**: Caa logika AI dziaa na Twoim komputerze dziki **Ollama**, zapewniajc prywatno i brak koszt贸w.
*    **Historia Konwersacji**: Wszystkie rozmowy s automatycznie zapisywane i mo偶na je wczytywa, usuwa oraz nadawa im tytuy generowane przez AI.
*    **Nowoczesny Interfejs**: Czysty, responsywny i przyjemny dla oka interfejs zbudowany w React.
*   锔 **Wasny "System Prompt"**: Mo偶liwo zdefiniowania "osobowoci" lub roli bota dla ka偶dej konwersacji.
*    **Obsuga Markdown i Kodu**: Odpowiedzi bota formatuj si, a bloki kodu maj podwietlan skadni.
*    **Przycisk Kopiuj**: atwe kopiowanie odpowiedzi bota jednym klikniciem.
*    **Monitor Zasob贸w**: Panel boczny na 偶ywo pokazuje zu偶ycie CPU, RAM i VRAM.

---

## Stack Technologiczny

*   **Frontend**: React (TypeScript), Vite, Tailwind CSS
*   **Backend**: Python 3.x, FastAPI, Uvicorn
*   **G贸wne Narzdzia**: Ollama

---

## Instalacja i Uruchomienie (Lokalne)

Procedura do wykonania **tylko za pierwszym razem**.

### Wymagania Wstpne

1.  **Node.js i npm**: [Pobierz i zainstaluj](https://nodejs.org/en/) (wersja LTS).
2.  **Python**: [Pobierz i zainstaluj](https://www.python.org/downloads/) (wersja 3.10+).
3.  **Ollama**: Postpuj zgodnie z instrukcj instalacji na [ollama.com](https://ollama.com/).

### Kroki Instalacji

1.  **Klonowanie Repozytorium**:
    ```bash
    git clone https://github.com/Flilipp/Chatbot
    cd chatbot-projekt
    ```

2.  **Konfiguracja Ollama**: Upewnij si, 偶e Ollama dziaa i pobierz model:
    ```bash
    ollama pull llama3
    ```

3.  **Instalacja Backendu (Terminal 1)**:
    ```bash
    cd backend
    python -m venv venv
    # Windows
    venv\Scripts\activate
    # macOS/Linux
    # source venv/bin/activate
    pip install -r requirements.txt
    ```

4.  **Instalacja Frontendu (Terminal 2)**:
    ```bash
    cd frontend
    npm install
    ```

---

## Jak Uruchomi Aplikacj Ponownie

To jest procedura, kt贸rej bdziesz u偶ywa na co dzie.

### Krok 1: Upewnij si, 偶e Ollama dziaa

Ollama powinna uruchamia si automatycznie w tle po starcie systemu. Jeli nie, uruchom j rcznie.

### Krok 2: Uruchom Backend

1.  Otw贸rz **pierwszy terminal** i przejd藕 do folderu projektu.
2.  Wejd藕 do folderu `backend` i aktywuj rodowisko wirtualne:
    ```bash
    cd backend
    # Windows
    venv\Scripts\activate
    # macOS/Linux
    # source venv/bin/activate
    ```
3.  Uruchom serwer:
    ```bash
    uvicorn api:app
    ```
    *Backend bdzie dziaa na `http://127.0.0.1:8000`. Zostaw ten terminal otwarty.*

### Krok 3: Uruchom Frontend

1.  Otw贸rz **drugi, osobny terminal** i przejd藕 do folderu projektu.
2.  Wejd藕 do folderu `frontend`:
    ```bash
    cd frontend
    ```
3.  Uruchom serwer deweloperski:
    ```bash
    npm run dev
    ```
    *Frontend bdzie dostpny w przegldarce pod adresem `http://localhost:8080/`. Zostaw ten terminal otwarty.*

### Gotowe!

Otw贸rz przegldark i wejd藕 na adres **`http://localhost:8080/`**.
