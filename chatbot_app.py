import streamlit as st
import ollama
import json
import os
from datetime import datetime
import psutil
from pynvml import *  # <--- TO JEST JEDYNY POPRAWNY IMPORT. ProszÄ™, upewnij siÄ™, Å¼e TwÃ³j plik go zawiera.
import re

# --- Konfiguracja i inicjalizacja ---

# Inicjalizacja biblioteki do monitorowania GPU NVIDIA
try:
    nvmlInit()
    GPU_SUPPORT = True
except NVMLError as error:
    GPU_SUPPORT = False

st.set_page_config(
    page_title="Lokalny Chatbot AI",
    page_icon="ðŸ¤–",
    layout="wide"
)

# Tworzenie folderu na historiÄ™ czatÃ³w, jeÅ›li nie istnieje
if not os.path.exists("chat_history"):
    os.makedirs("chat_history")


# --- Funkcje pomocnicze ---

def generate_chat_title(messages):
    """WysyÅ‚a zapytanie do modelu AI o wygenerowanie krÃ³tkiego tytuÅ‚u dla rozmowy."""
    messages_for_summary = list(messages)
    prompt = "Podsumuj tÄ™ rozmowÄ™ w 2 do 4 sÅ‚owach. To bÄ™dzie uÅ¼yte jako nazwa pliku. Odpowiedz tylko i wyÅ‚Ä…cznie samym tytuÅ‚em, bez Å¼adnych dodatkowych zdaÅ„ i znakÃ³w interpunkcyjnych."
    messages_for_summary.append({"role": "user", "content": prompt})
    try:
        response = ollama.chat(model='llama3', messages=messages_for_summary, stream=False)
        title = response['message']['content'].strip().replace('"', '').replace("'", "")
        return title if title else "Nowy czat"
    except Exception as e:
        print(f"BÅ‚Ä…d podczas generowania tytuÅ‚u: {e}")
        return "Nowy czat"


def create_safe_filename(title):
    """Tworzy bezpiecznÄ… nazwÄ™ pliku z tytuÅ‚u."""
    safe_title = re.sub(r'[^\w\s-]', '', title).strip().replace(' ', '_').lower()
    timestamp = datetime.now().strftime("%H%M%S")
    return f"{safe_title}_{timestamp}"


def get_system_stats():
    """Pobiera i zwraca statystyki zuÅ¼ycia CPU, RAM i VRAM."""
    cpu_usage = psutil.cpu_percent()
    ram_usage = psutil.virtual_memory().percent
    vram_usage = None
    if GPU_SUPPORT:
        try:
            handle = nvmlDeviceGetHandleByIndex(0)
            info = nvmlDeviceGetMemoryInfo(handle)
            vram_usage = (info.used / info.total) * 100
        except NVMLError as error:
            vram_usage = None
    return cpu_usage, ram_usage, vram_usage


def save_chat_history(chat_id, messages):
    """Zapisuje historiÄ™ czatu do pliku JSON."""
    file_path = os.path.join("chat_history", f"{chat_id}.json")
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(messages, f, ensure_ascii=False, indent=4)


def load_chat_history(chat_id):
    """Wczytuje historiÄ™ czatu z pliku JSON."""
    file_path = os.path.join("chat_history", f"{chat_id}.json")
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)


# --- GÅ‚Ã³wny interfejs aplikacji ---

st.title("ðŸ¤– Polski Chatbot AI")
st.caption("Oparty na modelu Llama 3 z historiÄ… rozmÃ³w i monitorem zasobÃ³w.")

# --- Panel boczny (Sidebar) ---

with st.sidebar:
    st.header("ZarzÄ…dzanie Czatami")

    if st.button("âž• Nowy Czat"):
        st.session_state.messages = [
            {"role": "system",
             "content": "JesteÅ› pomocnym asystentem AI. Zawsze odpowiadaj na pytania i prowadÅº konwersacjÄ™ wyÅ‚Ä…cznie w jÄ™zyku polskim."}
        ]
        st.session_state.active_chat_id = None
        st.rerun()

    st.subheader("Historia Konwersacji")

    try:
        chat_files = sorted(
            os.listdir("chat_history"),
            key=lambda x: os.path.getmtime(os.path.join("chat_history", x)),
            reverse=True
        )

        for chat_file in chat_files:
            chat_id = chat_file.replace(".json", "")
            col1, col2 = st.columns([3, 1])
            with col1:
                try:
                    last_underscore_index = chat_id.rindex('_')
                    display_name = chat_id[:last_underscore_index].replace("_", " ").capitalize()
                except ValueError:
                    display_name = chat_id.replace("_", " ").capitalize()

                if st.button(display_name, key=f"load_{chat_id}", use_container_width=True):
                    st.session_state.messages = load_chat_history(chat_id)
                    st.session_state.active_chat_id = chat_id
                    st.rerun()
            with col2:
                if st.button("ðŸ—‘ï¸", key=f"del_{chat_id}", help="UsuÅ„ ten czat"):
                    os.remove(os.path.join("chat_history", chat_file))
                    if st.session_state.get("active_chat_id") == chat_id:
                        st.session_state.messages = []
                        st.session_state.active_chat_id = None
                    st.rerun()
    except FileNotFoundError:
        st.info("Brak zapisanych rozmÃ³w.")

    st.header("Monitor ZasobÃ³w")

    cpu_usage, ram_usage, vram_usage = get_system_stats()

    st.metric(label="UÅ¼ycie CPU", value=f"{cpu_usage:.1f}%")
    st.metric(label="UÅ¼ycie RAM", value=f"{ram_usage:.1f}%")
    if vram_usage is not None:
        st.metric(label="UÅ¼ycie VRAM (GPU)", value=f"{vram_usage:.1f}%")
    elif GPU_SUPPORT is False:
        st.warning("Nie moÅ¼na zainicjowaÄ‡ biblioteki NVIDIA.")

# --- Inicjalizacja stanu sesji ---

if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "system",
         "content": "JesteÅ› pomocnym asystentem AI. Zawsze odpowiadaj na pytania i prowadÅº konwersacjÄ™ wyÅ‚Ä…cznie w jÄ™zyku polskim."}
    ]
if "active_chat_id" not in st.session_state:
    st.session_state.active_chat_id = None

# --- WyÅ›wietlanie wiadomoÅ›ci w gÅ‚Ã³wnym oknie ---

for message in st.session_state.messages:
    if message["role"] != "system":
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# --- GÅ‚Ã³wna logika wysyÅ‚ania i odbierania wiadomoÅ›ci ---

if prompt := st.chat_input("Napisz swojÄ… wiadomoÅ›Ä‡..."):
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        stream = ollama.chat(
            model='llama3',
            messages=[m for m in st.session_state.messages if m["role"] != "system"],
            stream=True,
        )

        for chunk in stream:
            full_response += chunk['message']['content']
            message_placeholder.markdown(full_response + "â–Œ")

        message_placeholder.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})

    is_new_chat = st.session_state.active_chat_id is None

    if is_new_chat:
        title = generate_chat_title(st.session_state.messages)
        new_chat_id = create_safe_filename(title)
        st.session_state.active_chat_id = new_chat_id

    save_chat_history(st.session_state.active_chat_id, st.session_state.messages)

    if is_new_chat:
        st.rerun()